{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T13:05:42.816155Z",
     "start_time": "2025-05-06T13:05:37.448354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "from v4.data.template import get_template_and_fix_tokenizer\n",
    "from v4.hparams import DataArguments\n",
    "from v4.data.converter import AlpacaDatasetConverter\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# 1. 初始化 tokenizer（确保路径和你的模型一致）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Math-1.5B-Instruct\")\n",
    "\n",
    "# 2. 指定使用的模板，比如 \"chatml\", \"llama2\", \"qwen\", 等等\n",
    "data_args = DataArguments(template=\"deepseek\")\n",
    "\n",
    "# 3. 为 converter 构造一个简化版 DatasetAttr（只要字段对应上即可）\n",
    "dataset_attr = SimpleNamespace(\n",
    "    prompt=\"instruction\",\n",
    "    query=\"input\",\n",
    "    response=\"output\",\n",
    "    history=None,\n",
    "    kto_tag=None,\n",
    "    ranking=False,\n",
    "    chosen=None,\n",
    "    rejected=None,\n",
    "    system=None,\n",
    "    tools=None,\n",
    "    images=None,\n",
    "    videos=None,\n",
    "    audios=None,\n",
    "    load_from=\"file\",\n",
    "    formatting=\"alpaca\",\n",
    ")\n",
    "\n",
    "# DataArguments 这里只为 media_dir 占位\n",
    "converter = AlpacaDatasetConverter(dataset_attr=dataset_attr, data_args=data_args)"
   ],
   "id": "680f57f99fecccbe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fzkuji\\.conda\\envs\\ensemble\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T13:10:15.864230Z",
     "start_time": "2025-05-06T13:10:15.858519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pprint\n",
    "\n",
    "# 你的原始样本\n",
    "sample = {\n",
    "    \"instruction\": \"Transform the following sentence using a synonym: The car sped quickly.\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"The car accelerated rapidly.\"\n",
    "}\n",
    "\n",
    "# 执行转换\n",
    "converted = converter(sample)\n",
    "\n",
    "print(\"---- 转换后格式（converted） ----\")\n",
    "pprint.pprint(converted)\n",
    "\n",
    "# 3. 获取模板并修复 tokenizer 的特殊 token\n",
    "template = get_template_and_fix_tokenizer(tokenizer, data_args)\n",
    "\n",
    "# converted[\"_prompt\"] 是一个列表，列表里每两个元素为 user/assistant 交替\n",
    "prompt_msgs = converted[\"_prompt\"]\n",
    "response_msgs = converted[\"_response\"]\n",
    "\n",
    "# 合并成 messages\n",
    "messages = prompt_msgs + response_msgs\n",
    "\n",
    "print(\"---- messages ----\")\n",
    "pprint.pprint(messages)\n",
    "\n",
    "# encode_oneturn 会把 messages 里的 user/assistant 按 template 转成 token_ids\n",
    "prompt_ids, response_ids = template.encode_oneturn(tokenizer, messages)\n",
    "\n",
    "print(\"\\n✅ 编码完成！\")"
   ],
   "id": "6feaa7bb64efdd90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 转换后格式（converted） ----\n",
      "{'_audios': None,\n",
      " '_images': None,\n",
      " '_prompt': [{'content': 'Transform the following sentence using a synonym: '\n",
      "                         'The car sped quickly.',\n",
      "              'role': 'user'}],\n",
      " '_response': [{'content': 'The car accelerated rapidly.',\n",
      "                'role': 'assistant'}],\n",
      " '_system': '',\n",
      " '_tools': '',\n",
      " '_videos': None}\n",
      "---- messages ----\n",
      "[{'content': 'Transform the following sentence using a synonym: The car sped '\n",
      "             'quickly.',\n",
      "  'role': 'user'},\n",
      " {'content': 'The car accelerated rapidly.', 'role': 'assistant'}]\n",
      "\n",
      "✅ 编码完成！\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T13:07:18.169611Z",
     "start_time": "2025-05-06T13:07:18.165969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [code]\n",
    "print(\"—— Prompt 解码 ——\")\n",
    "print(tokenizer.decode(prompt_ids, skip_special_tokens=False))\n",
    "\n",
    "print(\"\\n—— Response 解码 ——\")\n",
    "print(tokenizer.decode(response_ids, skip_special_tokens=False))\n"
   ],
   "id": "bca3d4e2d8ec4a48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—— Prompt 解码 ——\n",
      "User: Transform the following sentence using a synonym: The car sped quickly.\n",
      "\n",
      "Assistant:\n",
      "\n",
      "—— Response 解码 ——\n",
      "The car accelerated rapidly.<|im_end|>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "923c2daad5ba66d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
