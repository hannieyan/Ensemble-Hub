{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-10T11:25:06.398887Z"
    }
   },
   "source": [
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# -------------------------------\n",
    "# ÈÖçÁΩÆ\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "input_file = \"data/hendrycks_math_train.json\"\n",
    "acc_file = \"output/deepseek-r1-1.5b-generated-predictions-detailed-results.jsonl\"\n",
    "output_file = \"z_score/deepseek-r1-1.5b_ppl_conf_acc_z_scores_results.json\"\n",
    "\n",
    "# Ê®°ÂûãÁªüËÆ°Èáè\n",
    "model_stats = {\n",
    "    \"ppl_mean\": 9.795982360839844,\n",
    "    \"ppl_std\": 22.284496307373047,\n",
    "    \"conf_mean\": 0.6799513101577759,\n",
    "    \"conf_std\": 0.08082679659128189\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Âä†ËΩΩÊ®°ÂûãÂíå tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\").eval()\n",
    "\n",
    "# -------------------------------\n",
    "# Âä†ËΩΩÊï∞ÊçÆ\n",
    "with open(input_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(acc_file, \"r\") as f:\n",
    "    acc_data = [json.loads(line) for line in f]\n",
    "\n",
    "assert len(data) == len(acc_data), \"Ê†∑Êú¨Êï∞Èáè‰∏çÂåπÈÖç\"\n",
    "\n",
    "# -------------------------------\n",
    "# ËÆ°ÁÆó PPL Âíå Confidence\n",
    "def compute_ppl_and_conf(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"mps\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        ppl = math.exp(loss.item())\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        max_probs = probs.max(dim=-1).values\n",
    "        conf = max_probs[0, 1:-1].mean().item()  # exclude BOS & EOS\n",
    "\n",
    "    return ppl, conf\n",
    "\n",
    "# -------------------------------\n",
    "# ÊâπÈáèÂ§ÑÁêÜ\n",
    "results = []\n",
    "for sample, acc_record in tqdm(zip(data, acc_data), total=len(data)):\n",
    "    input_text = sample[\"input\"]\n",
    "    acc = 1 if acc_record.get(\"accuracy\", 0.0) >= 99.9 else 0\n",
    "\n",
    "\n",
    "    try:\n",
    "        ppl, conf = compute_ppl_and_conf(input_text)\n",
    "        z_ppl = (ppl - model_stats[\"ppl_mean\"]) / model_stats[\"ppl_std\"]\n",
    "        z_conf = (conf - model_stats[\"conf_mean\"]) / model_stats[\"conf_std\"]\n",
    "        results.append({\n",
    "            \"z_ppl\": z_ppl,\n",
    "            \"z_conf\": z_conf,\n",
    "            \"acc\": acc\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"error\": str(e),\n",
    "            \"acc\": acc\n",
    "        })\n",
    "\n",
    "# -------------------------------\n",
    "# ‰øùÂ≠òÁªìÊûú\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ ÂÆåÊàêÔºåÂ∑≤‰øùÂ≠òÂà∞: {output_file}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Batch-compute PPL / confidence / z-score / acc for multiple models.\n",
    "\"\"\"\n",
    "\n",
    "import json, math, os, gc, sys, logging\n",
    "from typing import Dict, List\n",
    "import torch, torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 0. ÁéØÂ¢É / Êó•Âøó\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[\\033[1;34m%(levelname)s\\033[0m] %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DEVICE  = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "DTYPE16 = torch.float16 if DEVICE == \"cuda\" else (torch.bfloat16 if torch.cuda.is_available() else None)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1. ÁªüËÆ°Èáè & Ê®°Âûã-Êï∞ÊçÆÈÖçÁΩÆ\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MODEL_STATS: Dict[str, Dict[str, float]] = {\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\": {\n",
    "        \"ppl_mean\": 9.795982360839844,  \"ppl_std\": 22.284496307373047,\n",
    "        \"conf_mean\": 0.6799513101577759, \"conf_std\": 0.08082679659128189,\n",
    "    },\n",
    "    \"Qwen/Qwen3-4B\": {\n",
    "        \"ppl_mean\": 6.160105228424072,  \"ppl_std\": 6.118084907531738,\n",
    "        \"conf_mean\": 0.8231604099273682, \"conf_std\": 0.07646501809358597,\n",
    "    },\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\": {\n",
    "        \"ppl_mean\": 16.57339096069336,  \"ppl_std\": 50.37682342529297,\n",
    "        \"conf_mean\": 0.6976740956306458, \"conf_std\": 0.10360505431890488,\n",
    "    },\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\": {\n",
    "        \"ppl_mean\": 8.22177505493164,  \"ppl_std\": 14.440741539001465,\n",
    "        \"conf_mean\": 0.7438507676124573, \"conf_std\": 0.0863514393568039,\n",
    "    },\n",
    "}\n",
    "\n",
    "data_name = \"aime24\"\n",
    "\n",
    "# ÈúÄÂ§ÑÁêÜÁöÑÊï∞ÊçÆ (‰∏ÄË°å‰∏Ä‰∏™ÈóÆÈ¢òÔºåÈ¢ÑÊµãÊòéÁªÜ„ÄÅËæìÂá∫Ë∑ØÂæÑ)\n",
    "DATASET = f\"data/{data_name}.json\"\n",
    "\n",
    "MODEL_SPECS: Dict[str, Dict[str, str]] = {\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\": {\n",
    "        \"acc_file\": f\"output/{data_name}/deepseek-r1-1.5b-generated-predictions-detailed-results.jsonl\",\n",
    "        \"out_file\": f\"z_score/{data_name}/deepseek-r1-1.5b_ppl_conf_acc_z_scores_results.json\",\n",
    "    },\n",
    "    \"Qwen/Qwen3-4B\": {\n",
    "        \"acc_file\": f\"output/{data_name}/qwen3-4b-generated-predictions-detailed-results.jsonl\",\n",
    "        \"out_file\": f\"z_score/{data_name}/qwen3-4b_ppl_conf_acc_z_scores_results.json\",\n",
    "    },\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\": {\n",
    "        \"acc_file\": f\"output/{data_name}/deepseek-r1-7b-generated-predictions-detailed-results.jsonl\",\n",
    "        \"out_file\": f\"z_score/{data_name}/deepseek-r1-7b_ppl_conf_acc_z_scores_results.json\",\n",
    "    },\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\": {\n",
    "        \"acc_file\": f\"output/{data_name}/deepseek-r1-14b-generated-predictions-detailed-results.jsonl\",\n",
    "        \"out_file\": f\"z_score/{data_name}/deepseek-r1-14b_ppl_conf_acc_z_scores_results.json\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2. ÂÖ¨Áî®ÂáΩÊï∞\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def compute_ppl_conf(model, tokenizer, text: str) -> Dict[str, float]:\n",
    "    \"\"\"return dict(ppl, conf)  ‚Äì conf = Âπ≥Âùá token-max-prob\"\"\"\n",
    "    enc = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        out = model(**enc, labels=enc[\"input_ids\"])\n",
    "        loss   = out.loss\n",
    "        logits = out.logits\n",
    "\n",
    "    ppl = math.exp(loss.item())\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    maxp  = probs.max(dim=-1).values[0]          # shape = (seq_len,)\n",
    "    # ÊéíÈô§ BOS / EOSÔºàÂÅáËÆæ tokenizer.eos_token_id Â≠òÂú®ÔºõÂ¶ÇÊûúÊó† eos_id ÂèØÂà†ÊéâÊúÄÂêé‰∏Ä‰∏™ tokenÔºâ\n",
    "    bos, eos = 0, -1\n",
    "    if tokenizer.bos_token_id is None: bos = None\n",
    "    if tokenizer.eos_token_id is None: eos = None\n",
    "    sel = maxp[1:eos] if bos is not None else maxp\n",
    "    conf = sel.mean().item()\n",
    "    return {\"ppl\": ppl, \"conf\": conf}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3. ‰∏ªÂæ™ÁéØ\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with open(DATASET, \"r\") as f:\n",
    "    dataset = json.load(f)                   # ÊØèÊù°Âê´ \"input\"\n",
    "\n",
    "logger.info(f\"Loaded {len(dataset)} problems from {DATASET}\")\n",
    "\n",
    "for model_name, paths in tqdm(MODEL_SPECS.items(), desc=\"Models\"):\n",
    "    logger.info(f\"\\n‚îÄ‚îÄ Processing [{model_name}] ‚îÄ‚îÄ\")\n",
    "    # 3.1 ËØª ACC ÊòéÁªÜ\n",
    "    with open(paths[\"acc_file\"], \"r\") as f:\n",
    "        acc_lines = [json.loads(x) for x in f]\n",
    "    assert len(acc_lines) == len(dataset), \"ACC file length mismatch\"\n",
    "\n",
    "    # 3.2 Âä†ËΩΩÊ®°Âûã / tokenizer\n",
    "    torch_dtype = DTYPE16 if DTYPE16 is not None else None\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True,\n",
    "    ).eval()\n",
    "\n",
    "    # 3.3 ÊâπÈáèËÆ°ÁÆó\n",
    "    stats   = MODEL_STATS[model_name]\n",
    "    results = []\n",
    "    for sample, acc_rec in tqdm(zip(dataset, acc_lines), total=len(dataset), leave=False):\n",
    "        acc = 1 if acc_rec.get(\"accuracy\", 0.0) >= 99.9 else 0\n",
    "        try:\n",
    "            m = compute_ppl_conf(model, tokenizer, sample[\"input\"])\n",
    "            z_ppl  = (m[\"ppl\"]  - stats[\"ppl_mean\"])  / stats[\"ppl_std\"]\n",
    "            z_conf = (m[\"conf\"] - stats[\"conf_mean\"]) / stats[\"conf_std\"]\n",
    "            results.append({\"z_ppl\": z_ppl, \"z_conf\": z_conf, \"acc\": acc})\n",
    "        except Exception as e:                          # ÊçïËé∑ÂçïÊù°ÂºÇÂ∏∏\n",
    "            results.append({\"error\": str(e), \"acc\": acc})\n",
    "\n",
    "    # 3.4 ‰øùÂ≠ò\n",
    "    os.makedirs(os.path.dirname(paths[\"out_file\"]), exist_ok=True)\n",
    "    with open(paths[\"out_file\"], \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    logger.info(f\"‚úÖ Saved {len(results)} records to {paths['out_file']}\")\n",
    "\n",
    "    # 3.5 ÈáäÊîæÊòæÂ≠ò\n",
    "    del model; del tokenizer; torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "logger.info(\"üü¢ All models finished.\")\n"
   ],
   "id": "5f80a117bc84cc7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7c8b59cbd3ff550"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
