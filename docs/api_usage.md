# Ensemble-Hub API ä½¿ç”¨æŒ‡å—

Enhanced API v2.0 æ”¯æŒçµæ´»çš„é›†æˆæ–¹æ³•é€‰æ‹©å’Œé…ç½®ã€‚

## ğŸš€ å¯åŠ¨ API æœåŠ¡å™¨

### åŸºç¡€å¯åŠ¨
```bash
# ä½¿ç”¨é»˜è®¤é…ç½®åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹å¯åŠ¨
python -m ensemblehub.api

# æˆ–ä½¿ç”¨ uvicornï¼ˆä»…æ”¯æŒæœåŠ¡å™¨é…ç½®ï¼Œä¸æ”¯æŒé›†æˆæ–¹æ³•é…ç½®ï¼‰
uvicorn ensemblehub.api:app --host 0.0.0.0 --port 8000
```

### å‘½ä»¤è¡Œé…ç½®å¯åŠ¨
**æ³¨æ„ï¼šé›†æˆæ–¹æ³•é…ç½®ä»…åœ¨ä½¿ç”¨ `python -m ensemblehub.api` å¯åŠ¨æ—¶æœ‰æ•ˆï¼Œuvicorn å¯åŠ¨æ–¹å¼ä¸æ”¯æŒè¿™äº›è‡ªå®šä¹‰å‚æ•°ã€‚**

```bash
# é…ç½®æœåŠ¡å™¨åœ°å€å’Œç«¯å£
python -m ensemblehub.api --host 0.0.0.0 --port 8080

# é…ç½®æ¨¡å‹é€‰æ‹©å’Œé›†æˆæ–¹æ³•
python -m ensemblehub.api --model_selection_method zscore --ensemble_method progressive

# é…ç½®å¾ªç¯æ¨ç†ï¼ˆä¸ä½¿ç”¨æ¨¡å‹é€‰æ‹©ï¼‰
python -m ensemblehub.api --model_selection_method all --ensemble_method loop --max_rounds 5

# é…ç½®æ¸è¿›å¼é›†æˆ
python -m ensemblehub.api --ensemble_method progressive --progressive_mode length \
  --length_thresholds 50,100,200 --max_rounds 3

# é…ç½®éšæœºé€‰æ‹©é›†æˆ
python -m ensemblehub.api --model_selection_method all --ensemble_method random --max_rounds 3

# é…ç½®å¾ªç¯é€‰æ‹©é›†æˆï¼ˆè½®è¯¢æ¨¡å¼ï¼‰
python -m ensemblehub.api --model_selection_method all --ensemble_method loop \
  --max_rounds 5 --max_repeat 2

# é…ç½®è‡ªå®šä¹‰æ¨¡å‹
python -m ensemblehub.api --model_specs '[{"path":"model1","engine":"hf"},{"path":"model2","engine":"hf"}]'

# æ˜¾ç¤ºæ¨¡å‹å½’å±ä¿¡æ¯
python -m ensemblehub.api --show_attribution

# æ˜¾ç¤ºè¯¦ç»†è¾“å…¥å‚æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
python -m ensemblehub.api --show_input_details

# å®Œæ•´é…ç½®ç¤ºä¾‹
python -m ensemblehub.api \
  --host 0.0.0.0 --port 8080 \
  --model_selection_method zscore \
  --ensemble_method progressive \
  --progressive_mode mixed \
  --length_thresholds 100,200 \
  --special_tokens "<step>,<think>" \
  --max_rounds 5 \
  --score_threshold -2.0 \
  --max_repeat 3 \
  --show_attribution \
  --show_input_details
```

### å¯ç”¨çš„å‘½ä»¤è¡Œå‚æ•°

#### æœåŠ¡å™¨é…ç½®
- `--host`: æœåŠ¡å™¨ä¸»æœºåœ°å€ (é»˜è®¤: 127.0.0.1)
- `--port`: æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 8000)

#### é›†æˆé…ç½®
- `--model_selection_method`: æ¨¡å‹é€‰æ‹©æ–¹æ³•
  - `zscore`: åŸºäº Z-score çš„ç»Ÿè®¡é€‰æ‹© (é»˜è®¤)
  - `all`: ä½¿ç”¨æ‰€æœ‰æ¨¡å‹
  - `random`: éšæœºé€‰æ‹©æ¨¡å‹
- `--ensemble_method`: é›†æˆæ–¹æ³•
  - `simple`: ç®€å•å¥–åŠ±æ¨¡å‹é›†æˆ (é»˜è®¤)
  - `progressive`: æ¸è¿›å¼é›†æˆ
  - `random`: éšæœºé›†æˆ
  - `loop`: å¾ªç¯/è½®è¯¢é›†æˆ
- `--max_rounds`: æœ€å¤§æ¨ç†è½®æ•° (é»˜è®¤: 10)
- `--score_threshold`: åˆ†æ•°é˜ˆå€¼ (é»˜è®¤: -1.5)
- `--max_repeat`: æœ€å¤§é‡å¤æ¬¡æ•° (é»˜è®¤: 3)

#### æ¸è¿›å¼é›†æˆç‰¹å®šé…ç½®
- `--progressive_mode`: æ¸è¿›æ¨¡å¼
  - `length`: åŸºäºé•¿åº¦çš„æ¨¡å‹åˆ‡æ¢
  - `token`: åŸºäºç‰¹æ®Šä»¤ç‰Œçš„æ¨¡å‹åˆ‡æ¢
  - `mixed`: æ··åˆæ¨¡å¼ (é»˜è®¤)
- `--length_thresholds`: é•¿åº¦é˜ˆå€¼åˆ—è¡¨ï¼Œé€—å·åˆ†éš” (å¦‚: 50,100,200)
- `--special_tokens`: ç‰¹æ®Šä»¤ç‰Œåˆ—è¡¨ï¼Œé€—å·åˆ†éš” (å¦‚: <step>,<think>)

#### æ¨¡å‹é…ç½®
- `--model_specs`: JSON æ ¼å¼çš„æ¨¡å‹è§„æ ¼åˆ—è¡¨

#### è°ƒè¯•å’Œè¾“å‡ºé…ç½®
- `--show_attribution`: æ˜¾ç¤ºæ¨¡å‹å½’å±ä¿¡æ¯ï¼ˆå“ªä¸ªæ¨¡å‹ç”Ÿæˆäº†å“ªéƒ¨åˆ†è¾“å‡ºï¼‰
- `--show_input_details`: æ˜¾ç¤ºè¯¦ç»†çš„è¾“å…¥å‚æ•°ï¼ˆç”¨äºè°ƒè¯• API è¯·æ±‚ï¼‰

æœåŠ¡å¯åŠ¨åè®¿é—®ï¼š
- API æ–‡æ¡£: http://localhost:8000/docs
- å¥åº·æ£€æŸ¥: http://localhost:8000/status

## ğŸ“‹ ä¸»è¦ API ç«¯ç‚¹

### 1. åŸºç¡€ä¿¡æ¯
- `GET /` - API ä¿¡æ¯å’Œç«¯ç‚¹åˆ—è¡¨
- `GET /status` - å¥åº·æ£€æŸ¥å’Œå¯ç”¨æ–¹æ³•
- `GET /v1/ensemble/methods` - åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„é›†æˆæ–¹æ³•

### 2. é…ç½®ç®¡ç†
- `GET /v1/ensemble/config` - è·å–å½“å‰é…ç½®
- `POST /v1/ensemble/config` - æ›´æ–°é…ç½®

### 3. æ¨ç†ç«¯ç‚¹
- `POST /v1/chat/completions` - OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆ
- `POST /v1/loop/completions` - ä¸“ç”¨å¾ªç¯æ¨ç†ç«¯ç‚¹ï¼ˆè½®è¯¢æ¨¡å¼ï¼‰
- `POST /v1/ensemble/inference` - ç›´æ¥é›†æˆæ¨ç†
- `POST /v1/ensemble/batch` - æ‰¹é‡æ¨ç†

### 4. é¢„è®¾ç«¯ç‚¹
- `POST /v1/ensemble/presets/simple` - ç®€å•é›†æˆ
- `POST /v1/ensemble/presets/selection_only` - ä»…æ¨¡å‹é€‰æ‹©
- `POST /v1/ensemble/presets/aggregation_only` - ä»…è¾“å‡ºèšåˆ

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€èŠå¤©å®Œæˆï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰

```bash
# ä½¿ç”¨ prompt å­—æ®µï¼ˆæ–‡æœ¬å®Œæˆæ ¼å¼ï¼‰
curl -X POST "http://localhost:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "ensemble",
  "prompt": "What is 2+2?",
  "max_tokens": 100
}'

# ä½¿ç”¨ messages å­—æ®µï¼ˆèŠå¤©æ ¼å¼ï¼‰
curl -X POST "http://localhost:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "ensemble",
  "messages": [
    {"role": "user", "content": "What is 2+2?"}
  ],
  "max_tokens": 100
}'
```

### 2. å¸¦é›†æˆé…ç½®çš„èŠå¤©å®Œæˆ

```bash
curl -X POST "http://localhost:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "ensemble",
  "messages": [
    {"role": "system", "content": "You are a helpful math tutor."},
    {"role": "user", "content": "Solve this math problem: 15 Ã— 23"}
  ],
  "max_tokens": 200,
  "ensemble_config": {
    "model_selection_method": "zscore",
    "aggregation_method": "reward_based",
    "aggregation_level": "sentence",
    "use_model_selection": true,
    "use_output_aggregation": true
  }
}'
```

### 3. æ¸è¿›å¼é›†æˆç¤ºä¾‹

#### åŸºäºé•¿åº¦çš„æ¸è¿›å¼é›†æˆ
```bash
curl -X POST "http://localhost:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "progressive-ensemble",
  "messages": [
    {"role": "system", "content": "You are a helpful math tutor."},
    {"role": "user", "content": "Explain how to solve 15 + 27"}
  ],
  "max_tokens": 256,
  "ensemble_config": {
    "ensemble_method": "progressive",
    "progressive_mode": "length",
    "length_thresholds": [500, 1000, 1500],
    "model_selection_method": "all",
    "show_attribution": true
  }
}'
```

#### åŸºäºç‰¹æ®Š Token çš„æ¸è¿›å¼é›†æˆ
```bash
curl -X POST "http://localhost:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "progressive-token",
  "messages": [
    {"role": "user", "content": "Think step by step: What is the derivative of x^2?"}
  ],
  "ensemble_config": {
    "ensemble_method": "progressive",
    "progressive_mode": "token",
    "special_tokens": ["<\\think>", "<\\analyze>"],
    "show_attribution": true
  }
}'
```

### 4. å¾ªç¯æ¨ç†ç«¯ç‚¹ï¼ˆè½®è¯¢æ¨¡å¼ï¼‰

```bash
curl -X POST "http://localhost:8000/v1/loop/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "ensemble",
  "prompt": "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",
  "max_tokens": 300,
  "ensemble_config": {
    "max_rounds": 5,
    "score_threshold": -1.5
  }
}'
```

### 5. æ‰¹é‡è¯·æ±‚ç¤ºä¾‹

#### æ‰¹é‡å¤„ç†å¤šä¸ªé—®é¢˜
```bash
curl -X POST "http://localhost:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "batch-ensemble",
  "messages": [
    [
      {"role": "user", "content": "What is 5 + 3?"}
    ],
    [
      {"role": "user", "content": "What is 10 * 7?"}
    ],
    [
      {"role": "system", "content": "You are a history expert."},
      {"role": "user", "content": "When was the Declaration of Independence signed?"}
    ]
  ],
  "max_tokens": 150,
  "ensemble_config": {
    "ensemble_method": "simple",
    "model_selection_method": "all",
    "show_attribution": true
  }
}'
```

#### ä½¿ç”¨ Legacy Prompt å­—æ®µçš„æ‰¹é‡è¯·æ±‚
```bash
curl -X POST "http://localhost:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "legacy-batch",
  "prompt": [
    "Calculate 8 * 9",
    "What is the capital of France?",
    "Explain photosynthesis in one sentence"
  ],
  "max_tokens": 100
}'
```

### 6. ç›´æ¥é›†æˆæ¨ç†

```bash
curl -X POST "http://localhost:8000/v1/ensemble/inference" \
-H "Content-Type: application/json" \
-d '{
  "instruction": "You are a helpful math tutor.",
  "input": "Explain how to solve quadratic equations",
  "ensemble_config": {
    "model_selection_method": "all",
    "aggregation_method": "reward_based",
    "aggregation_level": "sentence",
    "model_selection_params": {},
    "aggregation_params": {
      "max_repeat": 3
    }
  },
  "max_rounds": 10,
  "score_threshold": -1.5,
  "max_tokens": 500
}'
```

### 5. ä»…ä½¿ç”¨æ¨¡å‹é€‰æ‹©ï¼ˆä¸èšåˆè¾“å‡ºï¼‰

```bash
curl -X POST "http://localhost:8000/v1/ensemble/presets/selection_only" \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What is machine learning?",
  "model_selection_method": "zscore",
  "max_tokens": 300
}'
```

### 6. ä»…ä½¿ç”¨è¾“å‡ºèšåˆï¼ˆæ‰€æœ‰æ¨¡å‹ï¼‰

```bash
curl -X POST "http://localhost:8000/v1/ensemble/presets/aggregation_only" \
-H "Content-Type: application/json" \
-d '{
  "prompt": "Explain quantum computing",
  "aggregation_method": "round_robin",
  "aggregation_level": "sentence",
  "max_tokens": 400
}'
```

### 7. æ‰¹é‡æ¨ç†

```bash
curl -X POST "http://localhost:8000/v1/ensemble/batch" \
-H "Content-Type: application/json" \
-d '{
  "examples": [
    {
      "instruction": "You are a helpful assistant.",
      "input": "What is 5+5?",
      "ensemble_config": {
        "model_selection_method": "all",
        "aggregation_method": "reward_based"
      }
    },
    {
      "instruction": "You are a math expert.",
      "input": "What is 10Ã—10?",
      "ensemble_config": {
        "model_selection_method": "zscore",
        "aggregation_method": "random"
      }
    }
  ],
  "batch_size": 2
}'
```

## âš™ï¸ é…ç½®é€‰é¡¹

### æ¨¡å‹é€‰æ‹©æ–¹æ³• (model_selection_method)
- `"zscore"` - åŸºäº Z-score çš„æ¨¡å‹é€‰æ‹©ï¼ˆå›°æƒ‘åº¦å’Œç½®ä¿¡åº¦ï¼‰
- `"all"` - ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ¨¡å‹
- `"random"` - éšæœºé€‰æ‹©æ¨¡å‹å­é›†
- `"llm_blender"` - LLM-Blender æ–¹æ³•ï¼ˆå¦‚æœå®ç°ï¼‰

### è¾“å‡ºèšåˆæ–¹æ³• (aggregation_method)
- `"reward_based"` - åŸºäºå¥–åŠ±æ¨¡å‹åˆ†æ•°é€‰æ‹©è¾“å‡º
- `"random"` - éšæœºé€‰æ‹©ç”Ÿæˆçš„è¾“å‡º
- `"round_robin"` - è½®è¯¢é€‰æ‹©æ¨¡å‹è¾“å‡º

### èšåˆçº§åˆ« (aggregation_level)
- `"sentence"` - å¥å­/æ®µè½çº§åˆ«èšåˆï¼ˆç”Ÿæˆè¿‡ç¨‹ä¸­ï¼‰
- `"token"` - ä»¤ç‰Œçº§åˆ«èšåˆï¼ˆä¾‹å¦‚ GaCï¼‰
- `"response"` - å®Œæ•´å“åº”çº§åˆ«èšåˆï¼ˆä¾‹å¦‚æŠ•ç¥¨ï¼‰

## ğŸ”— Python å®¢æˆ·ç«¯ç¤ºä¾‹

### åŸºç¡€å®¢æˆ·ç«¯ç±»
```python
import requests
import json

class EnsembleClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
    
    def chat_completion(self, messages=None, prompt=None, ensemble_config=None, **kwargs):
        """å‘é€èŠå¤©å®Œæˆè¯·æ±‚"""
        payload = {
            "model": kwargs.get("model", "ensemble"),
            "max_tokens": kwargs.get("max_tokens", 256)
        }
        
        if messages:
            payload["messages"] = messages
        elif prompt:
            payload["prompt"] = prompt
        else:
            raise ValueError("Either messages or prompt must be provided")
            
        if ensemble_config:
            payload["ensemble_config"] = ensemble_config
            
        response = requests.post(f"{self.base_url}/v1/chat/completions", json=payload)
        return response.json()
    
    def batch_completion(self, conversations, ensemble_config=None, **kwargs):
        """æ‰¹é‡å¤„ç†å¤šä¸ªå¯¹è¯"""
        payload = {
            "model": "batch-ensemble",
            "messages": conversations,  # List[List[Message]]
            "max_tokens": kwargs.get("max_tokens", 256)
        }
        
        if ensemble_config:
            payload["ensemble_config"] = ensemble_config
        
        response = requests.post(f"{self.base_url}/v1/chat/completions", json=payload)
        return response.json()
```

### å•ä¸ªè¯·æ±‚ç¤ºä¾‹
```python
# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = EnsembleClient()

# åŸºç¡€èŠå¤©è¯·æ±‚
messages = [
    {"role": "user", "content": "What is artificial intelligence?"}
]
result = client.chat_completion(messages=messages)
print(result["choices"][0]["message"]["content"])

# ä½¿ç”¨æ¸è¿›å¼é›†æˆ
messages = [
    {"role": "user", "content": "Solve: 2x + 5 = 15"}
]

config = {
    "ensemble_method": "progressive",
    "progressive_mode": "length",
    "length_thresholds": [500, 1000],
    "show_attribution": True
}

result = client.chat_completion(messages=messages, ensemble_config=config)
print(json.dumps(result, indent=2))
```

### æ‰¹é‡è¯·æ±‚ç¤ºä¾‹
```python
# æ‰¹é‡å¤„ç†å¤šä¸ªé—®é¢˜
conversations = [
    [{"role": "user", "content": "What is 15 + 27?"}],
    [{"role": "user", "content": "Calculate 8 * 9"}],
    [{"role": "user", "content": "What is the square root of 144?"}]
]

config = {
    "ensemble_method": "progressive",
    "progressive_mode": "token",
    "special_tokens": ["<\\think>"],
    "show_attribution": True
}

batch_result = client.batch_completion(conversations, config)

# å¤„ç†ç»“æœ
for i, choice in enumerate(batch_result["choices"]):
    print(f"Conversation {i}:")
    print(f"  Response: {choice['message']['content']}")
    if choice.get("metadata", {}).get("attribution"):
        attr = choice["metadata"]["attribution"]
        print(f"  Attribution: {attr['summary']}")
    print()
```

## ğŸ“Š å“åº”æ ¼å¼

### å•ä¸ªè¯·æ±‚å“åº”æ ¼å¼
```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "created": 1710123456,
  "model": "ensemble",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Generated response text"
      },
      "finish_reason": "stop",
      "metadata": {
        "selected_models": ["Qwen/Qwen2.5-1.5B-Instruct", "Qwen/Qwen2.5-0.5B-Instruct"],
        "method": "all+progressive",
        "attribution": {
          "summary": "[R01:Qwen2.5-1.5B-Instruct] â†’ [R02:Qwen2.5-0.5B-Instruct]",
          "detailed": [
            {
              "text": "First part of response",
              "model": "Qwen/Qwen2.5-1.5B-Instruct",
              "round": 1,
              "length": 50
            },
            {
              "text": "Second part of response",
              "model": "Qwen/Qwen2.5-0.5B-Instruct",
              "round": 2,
              "length": 45
            }
          ]
        }
      }
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 95,
    "total_tokens": 105
  }
}
```

### æ‰¹é‡è¯·æ±‚å“åº”æ ¼å¼
æ‰¹é‡è¯·æ±‚ä¼šåœ¨ `choices` æ•°ç»„ä¸­è¿”å›å¤šä¸ªç»“æœï¼Œæ¯ä¸ªç»“æœå¯¹åº”ä¸€ä¸ªè¾“å…¥å¯¹è¯ï¼š

```json
{
  "id": "chatcmpl-batch-...",
  "object": "chat.completion",
  "created": 1710123456,
  "model": "batch-ensemble",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Answer to first question"
      },
      "finish_reason": "stop",
      "metadata": {...}
    },
    {
      "index": 1,
      "message": {
        "role": "assistant",
        "content": "Answer to second question"
      },
      "finish_reason": "stop",
      "metadata": {...}
    }
  ],
  "usage": {
    "prompt_tokens": 20,
    "completion_tokens": 100,
    "total_tokens": 120
  }
}
```

### Ensemble Inference å“åº”
```json
{
  "id": "ensemble-uuid",
  "created": 1234567890,
  "result": {
    "output": "ç”Ÿæˆçš„æ–‡æœ¬...",
    "selected_models": ["model1", "model2"],
    "method": "zscore+reward_based",
    "config": {...}
  },
  "config": {...}
}
```

## ğŸ› ï¸ è¿è¡Œæ—¶é…ç½®æ›´æ–°

```bash
# æ›´æ–°æ¨¡å‹é…ç½®
curl -X POST "http://localhost:8000/v1/ensemble/config" \
-H "Content-Type: application/json" \
-d '{
  "model_specs": [
    {"path": "new-model-1", "engine": "hf", "device": "cuda:0"},
    {"path": "new-model-2", "engine": "hf", "device": "cuda:1"}
  ]
}'

# æ›´æ–°é»˜è®¤é›†æˆé…ç½®
curl -X POST "http://localhost:8000/v1/ensemble/config" \
-H "Content-Type: application/json" \
-d '{
  "default_ensemble_config": {
    "model_selection_method": "zscore",
    "aggregation_method": "reward_based",
    "use_model_selection": true,
    "use_output_aggregation": true
  }
}'
```

## ğŸ§ª æµ‹è¯• API

```bash
# è¿è¡Œ API æµ‹è¯•
python test/test_api.py

# æˆ–æ‰‹åŠ¨æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://localhost:8000/status
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### vLLM CUDA å†…å­˜åˆ†é…é”™è¯¯

å¦‚æœä½ åœ¨ä½¿ç”¨ vLLM å¼•æ“æ—¶é‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
captures_underway.empty() INTERNAL ASSERT FAILED at "/pytorch/c10/cuda/CUDACachingAllocator.cpp":3085
```

**è§£å†³æ–¹æ¡ˆï¼š**

1. **ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ä¿®å¤ï¼ˆæ¨èï¼‰ï¼š**
   ```bash
   python -m ensemblehub.api --vllm_enforce_eager --vllm_disable_chunked_prefill
   ```

2. **åˆ‡æ¢åˆ° HuggingFace å¼•æ“ï¼š**
   ```bash
   # å°†æ¨¡å‹é…ç½®ä» "engine": "vllm" æ”¹ä¸º "engine": "hf"
   python -m ensemblehub.api --model_specs 'model_path:hf:cuda:0'
   ```

3. **ç¯å¢ƒå˜é‡è®¾ç½®ï¼š**
   ```bash
   export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
   python -m ensemblehub.api
   ```

### vLLM å±‚åç§°å†²çªé”™è¯¯

å¦‚æœä½ åœ¨ä½¿ç”¨ vLLM å¼•æ“æ—¶é‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
Duplicate layer name: model.layers.X.self_attn.attn
```

**è§£å†³æ–¹æ¡ˆï¼š**

1. **ä½¿ç”¨ä¼˜åŒ–çš„ vLLM é…ç½®ï¼ˆæ¨èï¼‰ï¼š**
   ```bash
   # ä½¿ç”¨ LlamaFactory é£æ ¼çš„é…ç½®ï¼Œé€‚åˆå•å¡å¤§æ¨¡å‹
   python -m ensemblehub.api --ensemble_method random --model_selection_method all
   ```

2. **åˆ‡æ¢åˆ° HuggingFace å¼•æ“ï¼ˆæœ€ç¨³å®šï¼‰ï¼š**
   ```bash
   # å°†æ¨¡å‹é…ç½®ä» "engine": "vllm" æ”¹ä¸º "engine": "hf" 
   python -m ensemblehub.api --model_specs 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B:hf:cuda:0'
   ```

3. **è°ƒè¯•æ¨¡å¼è®¾ç½®ï¼š**
   ```bash
   # å¦‚æœä»æœ‰é—®é¢˜ï¼Œä½¿ç”¨è°ƒè¯•æ¨¡å¼å¯åŠ¨
   export CUDA_LAUNCH_BLOCKING=1
   python -m ensemblehub.api --ensemble_method random
   ```

### HuggingFace Meta Tensor é”™è¯¯

å¦‚æœä½ åœ¨ä½¿ç”¨ HuggingFace å¼•æ“æ—¶é‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to()
```

**è§£å†³æ–¹æ¡ˆï¼š**

1. **ä½¿ç”¨ eager attentionï¼ˆæ¨èï¼‰ï¼š**
   ```bash
   python -m ensemblehub.api --hf_use_eager_attention
   ```

2. **ç¦ç”¨ device_mapï¼š**
   ```bash
   python -m ensemblehub.api --hf_disable_device_map
   ```

3. **é™çº§ transformers ç‰ˆæœ¬ï¼š**
   ```bash
   pip install transformers==4.35.0
   python -m ensemblehub.api
   ```

4. **ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…ï¼š**
   ```bash
   # å°†è®¾å¤‡ä» "cuda:X" æ”¹ä¸º "auto"
   python -m ensemblehub.api --model_specs 'model_path:hf:auto'
   ```

### GPU å†…å­˜ä¸è¶³é”™è¯¯

å¦‚æœä½ é‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
CUDA error: out of memory
```

**è§£å†³æ–¹æ¡ˆï¼š**

1. **ä½¿ç”¨é‡åŒ–å‡å°‘å†…å­˜å ç”¨ï¼ˆæ¨èï¼‰ï¼š**
   ```bash
   # ä½¿ç”¨ 8-bit é‡åŒ–
   python -m ensemblehub.api --hf_use_8bit
   
   # ä½¿ç”¨ 4-bit é‡åŒ–ï¼ˆæ›´èŠ‚çœå†…å­˜ï¼‰
   python -m ensemblehub.api --hf_use_4bit
   ```

2. **å‡å°‘åŒæ—¶åŠ è½½çš„æ¨¡å‹æ•°é‡ï¼š**
   ```bash
   # åªä½¿ç”¨è¾ƒå°çš„æ¨¡å‹
   python -m ensemblehub.api --model_specs 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B:hf:cuda:0,deepseek-ai/DeepSeek-R1-Distill-Qwen-7B:hf:cuda:1'
   ```

3. **ä½¿ç”¨ CPU è¿è¡Œå¤§æ¨¡å‹ï¼š**
   ```bash
   # å°†å¤§æ¨¡å‹ç§»åˆ° CPU ä¸Š
   python -m ensemblehub.api --model_specs 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B:hf:cpu'
   ```

4. **æ¸…ç† GPU ç¼“å­˜ï¼š**
   ```bash
   # åœ¨è¿è¡Œå‰æ¸…ç† GPU ç¼“å­˜
   export CUDA_LAUNCH_BLOCKING=1
   python -c "import torch; torch.cuda.empty_cache()"
   python -m ensemblehub.api
   ```

### å¸¸è§é—®é¢˜

**Q: API å¯åŠ¨åæ— æ³•è®¿é—®ï¼Ÿ**
A: æ£€æŸ¥é˜²ç«å¢™è®¾ç½®ï¼Œç¡®ä¿ç«¯å£æœªè¢«å ç”¨ï¼š
```bash
curl http://localhost:8000/status
```

**Q: æ¨¡å‹åŠ è½½å¤±è´¥ï¼Ÿ**
A: æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œè®¾å¤‡é…ç½®ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜ï¼š
```bash
nvidia-smi  # æ£€æŸ¥ GPU ä½¿ç”¨æƒ…å†µ
```

**Q: é›†æˆæ–¹æ³•é…ç½®æ— æ•ˆï¼Ÿ**
A: ç¡®ä¿ä½¿ç”¨ `python -m ensemblehub.api` è€Œä¸æ˜¯ `uvicorn` æ¥å¯åŠ¨ï¼š
```bash
# âœ… æ­£ç¡®
python -m ensemblehub.api --ensemble_method loop

# âŒ ä¸æ”¯æŒè‡ªå®šä¹‰é…ç½®
uvicorn ensemblehub.api:app --host 0.0.0.0 --port 8000
```

## ğŸ”— lm-evaluation-harness å…¼å®¹æ€§

Ensemble-Hub API å®Œå…¨å…¼å®¹ lm-evaluation-harnessï¼Œæ”¯æŒæ‰€æœ‰æ ‡å‡†å‚æ•°ï¼š

### ä½¿ç”¨ lm-eval æµ‹è¯•

```bash
# åŸºç¡€æµ‹è¯•
lm_eval --model openai-completions \
  --tasks gsm8k \
  --model_args model=ensemble,base_url=http://localhost:8000/v1/chat/completions,tokenizer_backend=None \
  --batch_size 2 \
  --num_fewshot 5

# å®Œæ•´å‚æ•°ç¤ºä¾‹
OPENAI_API_KEY=dummy lm_eval \
  --model openai-completions \
  --tasks gsm8k,hendrycks_math \
  --model_args model=ensemble,base_url=http://localhost:8000/v1/chat/completions,tokenizer_backend=None \
  --batch_size 16 \
  --num_fewshot 5 \
  --limit 100
```

### æ”¯æŒçš„ lm-eval å‚æ•°

API å®Œå…¨æ”¯æŒä»¥ä¸‹ lm-evaluation-harness å‚æ•°ï¼š

- **`max_tokens`**: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
- **`temperature`**: é‡‡æ ·æ¸©åº¦ï¼ˆ0 è¡¨ç¤ºè´ªå©ªè§£ç ï¼‰
- **`stop`**: åœæ­¢åºåˆ—åˆ—è¡¨ï¼ˆå¦‚ `["Question:", "</s>", "<|im_end|>"]`ï¼‰
- **`seed`**: éšæœºç§å­ï¼Œç”¨äºç»“æœå¯é‡ç°æ€§

### è°ƒè¯• lm-eval è¯·æ±‚

å½“éœ€è¦è°ƒè¯• lm-evaluation-harness å‘é€çš„è¯·æ±‚æ—¶ï¼š

```bash
# å¯åŠ¨ API å¹¶æ˜¾ç¤ºè¾“å…¥è¯¦æƒ…
python -m ensemblehub.api --show_input_details

# è¿è¡Œ lm-evalï¼ŒAPI æ—¥å¿—å°†æ˜¾ç¤ºå®Œæ•´è¯·æ±‚å†…å®¹
```

ç¤ºä¾‹æ—¥å¿—è¾“å‡ºï¼š
```
================================================================================
Received API request:
Model: ensemble
Messages: None
Prompt: Question: Janet's ducks lay 16 eggs per day...
Temperature: 0.0
Max tokens: 256
Stop: ['Question:', '</s>', '<|im_end|>']
Seed: 1234
================================================================================
```

## ğŸ”„ è‡ªåŠ¨æ‰¹é‡æ£€æµ‹è§„åˆ™

API ä¼šè‡ªåŠ¨æ£€æµ‹è¯·æ±‚ç±»å‹ï¼Œæ— éœ€ä½¿ç”¨ä¸åŒçš„ç«¯ç‚¹ï¼š

1. **å•ä¸ªè¯·æ±‚**: 
   - `messages` æ˜¯ `List[Message]` æ ¼å¼
   - `prompt` æ˜¯å•ä¸ªå­—ç¬¦ä¸²

2. **æ‰¹é‡è¯·æ±‚**: 
   - `messages` æ˜¯ `List[List[Message]]` æ ¼å¼
   - `prompt` æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ `List[str]`

åŒä¸€ä¸ª `/v1/chat/completions` ç«¯ç‚¹å¯ä»¥å¤„ç†æ‰€æœ‰æƒ…å†µï¼Œè‡ªåŠ¨è¯†åˆ«å¹¶æ­£ç¡®å¤„ç†ã€‚

è¿™ä¸ªå¢å¼ºçš„ API æä¾›äº†å®Œå…¨çš„çµæ´»æ€§ï¼Œè®©ä½ å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©å’Œé…ç½®ä¸åŒçš„é›†æˆæ–¹æ³•ï¼