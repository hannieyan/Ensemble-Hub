## **Ensemble-Inference ğŸš€**

  

_Multi-model reasoning with reward-guided selection, notebook & API ready._

---

### **ğŸŒŸ Project goals**

|**Why?**|**How?**|
|---|---|
|**Boost answer quality** by letting several LLMs â€œcompeteâ€.|Each round, every generator writes a short segment â†’ a reward model (Qwen 2.5-Math-PRM-7B) scores them â†’ best segment is kept.|
|**Stay fast & memory-friendly** with model caching.|ModelPool loads each generator/reward model once, then re-uses it for every call (CLI, notebook or API).|
|**Provide plug-and-play usage** for research & services.|Python helper run_ensemble() **or** a production-grade FastAPI server (ensemble_api_server.py).|

  

---

### **ğŸ—‚ Repository layout**

```
ensemble-inference/
â”œâ”€â”€ ensemble_inference.py       # Core logic (cached pool, template, EOS early-stop)
â”œâ”€â”€ ensemble_api_server.py      # FastAPI wrapper
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ example.yaml            # Demo config â€“ three DeepSeek models + reward
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ quick_demo.ipynb        # Colab/Jupyter walkthrough
â”œâ”€â”€ requirements.txt            # Minimal deps
â””â”€â”€ README.md                   # â†’ you are here
```

  

---

### **ğŸ”§ Environment**

|**Package**|**Version tested**|**Notes**|
|---|---|---|
|Python|â‰¥ 3.9||
|PyTorch|â‰¥ 2.2|+ CUDA 11/12 GPU recommended|
|transformers|â‰¥ 4.40|HF backend|
|fastapi & uvicorn|â‰¥ 0.110|API server|
|pyyaml|any|load config.yaml|
|**Optional**|||
|vllm|â‰¥ 0.4|ultra-fast inference backend|

```
# CUDA-enabled install (edit as needed)
pip install -r requirements.txt
# add vLLM if you want that backend
pip install vllm
```

  

---

### **â–¶ï¸ Quick start (Python / notebook)**

```
from ensemble_inference import run_ensemble

model_specs = [
    {"path": "/models/DeepSeek-R1-Distill-Qwen-1.5B",  "engine": "hf"},
    {"path": "/models/DeepSeek-R1-Distill-Qwen-7B",   "engine": "hf"},
    {"path": "/models/DeepSeek-R1-Distill-Qwen-14B",  "engine": "vllm"},
]

answer = run_ensemble(
    "Explain gradient accumulation in simple terms.",
    model_specs=model_specs,
    max_rounds=5,
    accumulate_context=True      # let the conversation build
)
print(answer)
```

_Under the hood: models are pulled once â†’ PRM scores each round â†’ loop stops when a selected segment ends with its own EOS token._

---

### **ğŸ›° Start the REST API**

1. **Create a YAML config** (configs/example.yaml)
```
models:
  - path: /models/DeepSeek-R1-Distill-Qwen-1.5B
    engine: hf
  - path: /models/DeepSeek-R1-Distill-Qwen-7B
    engine: hf
reward_path: /models/Qwen2.5-Math-PRM-7B
```

2. **Launch**

```
python ensemble_api_server.py \
    --config configs/example.yaml \
    --host 0.0.0.0 --port 8000
```

3. **Ping**
```
curl http://localhost:8000/status
# âœ {"status":"ready"}
```

4. **Ask a question**
```
curl -X POST http://localhost:8000/api/generate \
     -H "Content-Type: application/json" \
     -d '{"question":"What is RLHF?", "max_rounds":4}'
```


---

### **ğŸ’¡ Core features**

- **Unlimited generators** â€“ mix HF & vLLM backends (engine: hf|vllm).
- **Reward-guided selection** â€“ Qwen 2.5-Math-PRM-7B, official step-probability scoring.
- **EOS-based early stop** â€“ each modelâ€™s own eos_token_id triggers loop exit.
- **Context accumulation** (opt-in/out).
- **Clean prompt template** â€“ no stray â€œ600 wordsâ€ artefacts.
- **Singleton caches** â€“ zero reload on repeated calls, even across API requests.

---

### **âœï¸ Extending**

- **More backends** â€“ subclass BaseGenerator, register in ModelPool.
- **Streaming answers** â€“ wrap run_ensemble in an async generator & return SSE / websockets.
- **Custom reward models** â€“ implement PRMScorer-like class & swap in ModelPool.get_reward.

---

### **ğŸ“œ License**

Apache-2.0. See LICENSE.

---

### **ğŸ™ Acknowledgements**

Relies on **DeepSeek**, **Qwen** model weights, HuggingFace Transformers and the incredible open-source community.