{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "\n",
    "# Optional vLLM backend ------------------------------------------------------\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams  # type: ignore\n",
    "\n",
    "    _VLLM_AVAILABLE = True\n",
    "except ImportError:  # pragma: no cover\n",
    "    _VLLM_AVAILABLE = False\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Logging / constants\n",
    "# ---------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(\"ensemble_inference\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EOS_TEXT = \"\"  # most Qwen / Llama models use empty string as EOS\n",
    "STEP_TOKEN = \"<extra_0>\"\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "STOP_TOKENS_TEXT = {\".\", \"\\n\"}  # trimming convenience\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Conversation template & sanitisation\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# def _clean(text: str) -> str:\n",
    "#     \"\"\"Strip trivial meta lines like \"600 words\".\"\"\"\n",
    "#     return re.sub(r\"^\\s*\\d+\\s*words\\b.*(?:\\n|$)\", \"\", text, flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "\n",
    "\n",
    "class ConversationTemplate:\n",
    "    def __init__(self, system_prompt: str, initial_question: str):\n",
    "        self.system = system_prompt\n",
    "        self.turns: List[Tuple[str, str]] = [(\"user\", initial_question)]  # (role, content)\n",
    "\n",
    "    def add_assistant(self, content: str):\n",
    "        self.turns[-1] = (self.turns[-1][0], self.turns[-1][1])  # ensure last user ends\n",
    "        self.turns.append((\"assistant\", content))\n",
    "\n",
    "    def new_turn(self, question: str):\n",
    "        self.turns.append((\"user\", question))\n",
    "\n",
    "    def render(self) -> str:\n",
    "        prompt_lines = [f\"[SYSTEM] {self.system} [/SYSTEM]\"]\n",
    "        for idx, (role, content) in enumerate(self.turns, 1):\n",
    "            if role == \"user\":\n",
    "                prompt_lines.append(f\"[TURN {idx}]\\n<user>\\n{content}\\n</user>\")\n",
    "            else:  # assistant\n",
    "                prompt_lines.append(f\"<assistant>\\n{content}\\n</assistant>\")\n",
    "        prompt_lines.append(\"<assistant>\\n\")  # leave open for next gen\n",
    "        return \"\\n\".join(prompt_lines)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helper: trim at first stop token\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _trim_text(txt: str) -> str:\n",
    "    for tok in STOP_TOKENS_TEXT:\n",
    "        pos = txt.find(tok)\n",
    "        if pos != -1:\n",
    "            return txt[: pos + len(tok)]\n",
    "    return txt\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Step‑probability helper for reward model\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _step_rewards(logits: torch.Tensor, mask: torch.Tensor):\n",
    "    probs = F.softmax(logits, dim=-1) * mask.unsqueeze(-1)\n",
    "    arr: List[List[float]] = []\n",
    "    for sample in probs:\n",
    "        pos = sample[sample != 0].view(-1, 2)[:, 1]\n",
    "        arr.append(pos.cpu().tolist())\n",
    "    return arr\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Generator output container\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GenOutput:\n",
    "    text: str\n",
    "    ended_with_eos: bool\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Base Generator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class BaseGenerator:\n",
    "    name: str\n",
    "\n",
    "    def generate(self, prompt: str, **kw) -> GenOutput:  # pragma: no cover abstract\n",
    "        raise NotImplementedError\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HF Generator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class HFGenerator:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, model: AutoModelForCausalLM):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.name = \"\"\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str, *, dtype: torch.dtype = torch.float16):\n",
    "        tok = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "        mod = AutoModelForCausalLM.from_pretrained(path, torch_dtype=dtype, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "        inst = cls(tok, mod)\n",
    "        inst.name = path\n",
    "        return inst\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, *, max_tokens=128, temperature=0.95, top_p=0.7) -> GenOutput:\n",
    "        ids = self.tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        cfg = GenerationConfig(\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            stop_strings=list(STOP_TOKENS_TEXT) + [self.tokenizer.decode([self.tokenizer.eos_token_id], skip_special_tokens=True)]\n",
    "        )\n",
    "        out = self.model.generate(**ids, generation_config=cfg, tokenizer=self.tokenizer)[0]\n",
    "        ended = bool(out[-1] == self.tokenizer.eos_token_id)\n",
    "\n",
    "        txt = self.tokenizer.decode(out[len(ids[\"input_ids\"][0]) :], skip_special_tokens=False)\n",
    "        print(txt)\n",
    "        return GenOutput(txt, ended)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# vLLM Generator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class VLLMGenerator:\n",
    "    def __init__(self, path: str):\n",
    "        if not _VLLM_AVAILABLE:\n",
    "            raise RuntimeError(\"vllm not installed\")\n",
    "        self._llm = LLM(model=path)\n",
    "        self._sp = SamplingParams(max_tokens=128, temperature=0.95, top_p=0.7, stop=list(STOP_TOKENS_TEXT))\n",
    "        self.name = path\n",
    "        self._eos_text = EOS_TEXT\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, *, max_tokens=128, temperature=0.95, top_p=0.7) -> GenOutput:\n",
    "        self._sp.max_tokens, self._sp.temperature, self._sp.top_p = max_tokens, temperature, top_p\n",
    "        txt = self._llm.generate([prompt], self._sp)[0].outputs[0].text\n",
    "        ended = txt.endswith(self._eos_text)\n",
    "        return GenOutput(_trim_text(txt), ended)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ModelPool: caching layer\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ModelPool:\n",
    "    _gen_cache: Dict[Tuple[str, str], BaseGenerator] = {}\n",
    "    _reward_cache: Dict[str, \"PRMScorer\"] = {}\n",
    "\n",
    "    @classmethod\n",
    "    def get_generator(cls, path: str, engine: str = \"hf\") -> BaseGenerator:\n",
    "        key = (engine, path)\n",
    "        if key not in cls._gen_cache:\n",
    "            logger.info(\"[Pool] loading %s (%s)\", path, engine)\n",
    "            if engine == \"hf\":\n",
    "                cls._gen_cache[key] = HFGenerator.load(path)\n",
    "            elif engine == \"vllm\":\n",
    "                cls._gen_cache[key] = VLLMGenerator(path)\n",
    "            else:\n",
    "                raise ValueError(engine)\n",
    "        return cls._gen_cache[key]\n",
    "\n",
    "    @classmethod\n",
    "    def get_reward(cls, path: str) -> \"PRMScorer\":\n",
    "        if path not in cls._reward_cache:\n",
    "            logger.info(\"[Pool] loading reward model %s\", path)\n",
    "            cls._reward_cache[path] = PRMScorer(path)\n",
    "        return cls._reward_cache[path]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Reward model\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class PRMScorer:\n",
    "    def __init__(self, path: str):\n",
    "        self.tok = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "        self.mod = AutoModel.from_pretrained(path, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "        self.sep_id = self.tok.encode(STEP_TOKEN)[0]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def score(self, question: str, answer: str) -> float:\n",
    "        if not answer.endswith(STEP_TOKEN):\n",
    "            answer = answer + STEP_TOKEN\n",
    "        msgs = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        convo = self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
    "        ids = self.tok(convo, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "        mask = ids == self.sep_id\n",
    "        probs = _step_rewards(self.mod(ids).logits, mask)[0]\n",
    "        return float(sum(probs) / len(probs) * 10.0) if probs else 0.0\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ensemble reasoner\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class EnsembleReasoner:\n",
    "    def __init__(self, generators: List[BaseGenerator], scorer: PRMScorer, max_rounds: int = 5,\n",
    "                 score_threshold: float = 0.5, accumulate_context: bool = True):\n",
    "        self.generators = generators\n",
    "        self.scorer = scorer\n",
    "        self.max_rounds = max_rounds\n",
    "        self.score_threshold = score_threshold\n",
    "        self.accumulate_context = accumulate_context\n",
    "\n",
    "    def __call__(self, question: str) -> str:\n",
    "        convo = ConversationTemplate(SYSTEM_PROMPT, question)\n",
    "        collected: List[str] = []\n",
    "\n",
    "        for rnd in range(1, self.max_rounds + 1):\n",
    "            prompt = convo.render()\n",
    "            outs = [g.generate(prompt) for g in self.generators]\n",
    "            segs = [o.text for o in outs]\n",
    "            scores = [self.scorer.score(question, s) for s in segs]\n",
    "\n",
    "            for g, t, s in zip(self.generators, segs, scores):\n",
    "                logger.info(\"→ %s | %.2f | %s\", g.name, s, t.replace(\"\\n\", \"\\\\n\"))\n",
    "\n",
    "            best_idx = int(torch.tensor(scores).argmax())\n",
    "            best_out = outs[best_idx]\n",
    "            best_score = scores[best_idx]\n",
    "\n",
    "            if best_score < self.score_threshold:\n",
    "                logger.info(\"Stop: best score %.2f < threshold\", best_score)\n",
    "                break\n",
    "\n",
    "            collected.append(best_out.text)\n",
    "            if self.accumulate_context:\n",
    "                convo.add_assistant(best_out.text)\n",
    "            if best_out.ended_with_eos:\n",
    "                logger.info(\"Early stop: EOS token emitted\")\n",
    "                break\n",
    "\n",
    "        return \" \".join(collected)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
