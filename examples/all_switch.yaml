# Ensemble-Hub API Configuration
# This file contains all configuration parameters for the API server

# Server Configuration
server:
  host: "0.0.0.0"
  port: 8000

# Debug Settings
debug:
  show_input_details: false  # Show raw HTTP request body in logs
  show_output_details: false  # Show detailed output results in logs
  save_results: false  # Save results to disk (useful for debugging)

# Model Specifications
# Format: path, engine (hf/vllm/hf_rm/hf_gen/api), and optional parameters
# Note: For HF models, device is controlled via max_memory parameter
model_specs:
  - path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
    engine: "hf"
    # max_memory: null  # null means auto device allocation
    # quantization: "none"  # Options: none, 8bit, 4bit
    enable_thinking: false
    padding_side: "right"  # DeepSeek models use right padding
    num_gpus: 0.7  # GPU resources to allocate (0.5 for shared GPU, 1.0 for full GPU, 2 or more for large models)
  
  - path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    engine: "hf"
    # max_memory: {"0": "10GB", "cpu": "20GB"}  # Example: specific memory allocation
    num_gpus: 0.3  # Use half GPU for smaller models
    enable_thinking: false
    padding_side: "right"  # DeepSeek models use right padding
  
  # Additional model examples (uncomment to use):
  # - path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
  #   engine: "hf"
  #   quantization: "4bit"
  #   max_memory: {"0": "4GB", "1": "4GB"}  # Multi-GPU example
  #   num_gpus: 0.5  # Smaller models can share GPU resources
  
  # - path: "Qwen/Qwen2.5-Math-7B-Instruct"
  #   engine: "vllm"
  #   device: "cuda:1"  # vLLM still uses device parameter

# Default Ensemble Configuration
ensemble:
  # Model Selection Method
  # Options: all, zscore, model_judgment, random
  model_selection_method: "all"
  model_selection_params: {}
  
  # Output Aggregation Method
  # Options: loop, progressive, random, reward_based
  output_aggregation_method: "switch"
  output_aggregation_params:
    switch_after_tokens: 50  # Maximum tokens for outline generation
  
  # Default generation parameters (can be overridden by API request)
  generation:
    max_tokens: 60  # Total output length (must be larger than outline_max_tokens)
    temperature: 1.0
    do_sample: false  # Enable sampling for more diverse outputs
    top_p: 1.0
    presence_penalty: 0.0
    frequency_penalty: 0.0
    stop_strings: []

# Engine-specific Options
engine_options:

  # HuggingFace Options
  hf:
    use_eager_attention: true  # Use eager attention implementation (fixes meta tensor errors)
    disable_device_map: false  # Disable device_map for specific device assignment
    use_8bit: false  # Global 8-bit quantization setting
    use_4bit: false  # Global 4-bit quantization setting
    low_cpu_mem: true  # Use low CPU memory loading
